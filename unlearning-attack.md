### 📚 Unlearning Attack 论文分类总览表

| 论文标题                                                     | 作者                                                         |       会议/年份        | 遗忘类型                                | 攻击场景 (Attack Scenario)                                   | 攻击类型 (Attack Type)                       | 威胁模型 (Threat Model)                                      | 关键方法/贡献简述                                            | 是否开源                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ | :--------------------: | --------------------------------------- | ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------------------- |
| `Paper Title`                                                | `Author et al.`                                              |      `Conf. Year`      | `exact/approximate unlearning`          | `e.g., 单样本遗忘 / 联邦遗忘 / 在线学习`                     | `e.g., MIA / 模型提取 / 遗忘绕过 / 差分攻击` | `e.g., 黑盒 / 白盒 / 灰盒`                                   | 简要描述其攻击机制或创新点（如：“提出针对SISA框架的MIA变体”） | ✅ / ❌                                                      |
| `A Duty to Forget, a Right to be Assured? Exposing  Vulnerabilities in Machine Unlearning Services` | `Hongsheng Hu∗, Shuo Wang§∗, Jiamin Chang†∗, Haonan Zhong†∗, Ruoxi Sun∗, Shuang Hao‡, Haojin Zhu§, and Minhui Xue∗` |      `NDSS 2024`       | `gradient-based approximate unlearning` | `提交恶意的遗忘申请（修改数据）诱导服务器模型的严重性能下降` | `数据中毒攻击`                               | `黑盒(提交数据样本x来查询模型，并获得概率向量Y，但无法知道模型的参数和结构)不知道unlearnng算法` | `利用CW-attack生产对抗样本`                                  | ` https://github.com/ TASI-LAB/Over-unlearning`            |
| `Backdoor Attacks via Machine Unlearning`                    | `Zihao Liu1, Tianhao Wang2, Mengdi Huai1, Chenglin Miao1`    |      `AAAI 2024`       | `exact && approximate unlearning`       | `通过使用unlearning擦除其部分训练数据来使目标模型表现出后门行为` | `后门攻击`                                   | `黑盒（）&&白盒（毒害训练集）都知道unlearning算法`           | `数学公式优化推导触发器及数据集，离散的样本选择转换为连续的概率` | ❌                                                          |
| `FedMUA: Exploring the Vulnerabilities of  Federated Learning to Malicious  Unlearning Attacks` | `Jian Chen , Member, IEEE, Zehui Lin, Wanyu Lin ,  Wenlong Shi, Xiaoyan Yin , Member, IEEE, and Di Wang , ` |      `TIFS 2025`       | `exact && approximate unlearning`       | `发起恶意的特征遗忘请求来显著地改变与目标样本相关的预测`     | `数据中毒攻击`                               | `黑盒（本地客户端训练数据中的小比例数据）`                   | `利用影响函数识别最具影响力的样本，扰动这些样本使靠近目标样本` | ` https://github.com/ity207/FedMUA`                        |
| `Hard to Forget: Poisoning Attacks on Certified Machine Unlearning` | `Marchant, N. G., Rubinstein, B. I. P., & Alfeld, S.  `      |      `AAAI 2022`       | ` approximate unlearning`               | `发起恶意的遗忘请求使模型slow-down（触发重训练）`            | `数据中毒攻击`                               | `白盒设置中，访问良性用户的训练数据、部署模型的架构和模型状态。灰盒设置中，仍然拥有模型架构的知识，但不能再访问良性用户的训练数据和模型状态。` | `PGD-based crafting`                                         | ` https://github.com/ngmarchant/attack-unlearning`         |
| `Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning` | `Hongsheng Hu∗, Shuo Wang†, Tian Dong† and Minhui Xue∗`      |       `S&P 2024`       | `exact && approximate unlearning`       | `只访问原始和未学习的模型来揭示遗忘样本的特征和标签信息`     | `Inversion Attacks（实验设置存在问题）`      | `白盒设置（feature）/黑盒设置（label）不需要unlearning算法和数据集信息` | `梯度估计优化（余弦+总变分）对抗样本生成检测模型logit差异`   | `https://github.com/TASI-LAB/Unlearning-inversion-attacks` |
| `MACHINE UNLEARNING FAILS TO REMOVE  DATA POISONING ATTACKS` | `Martin Pawelczyk,Jimmy Z. Di, Yiwei Lu Ayush Sekhari, Gautam Kamath Seth Neel` |      `ICLR 2025`       | `approximate unlearning`                |                                                              | `数据中毒攻击`                               |                                                              |                                                              |                                                            |
| `UBA-Inf: Unlearning Activated Backdoor Attack with Influence-Driven Camouflage` | `Zirui Huang   Yunlong Mao∗   Sheng Zhong`                   | `USENIX Security 2024` | `exact && approximate unlearning`       | `云上模型训练中植入有毒样本，持续学习场景下发送unlearning请求激活后门` | `后门攻击`                                   | `灰盒：MLaaS 的云端模型（黑盒）有辅助数据集构造影子模型`     | `利用影响函数构造伪装样本`                                   | `https://github.com/Huangzirui1206/UBA-Inf/releases`       |
| `ReVeil: Unconstrained Concealed Backdoor Attack on  Deep Neural Networks using Machine Unlearning` | `Manaar Alam, Hithem Lamri, and Michail Maniatakos`          |       `DAC 2025`       | `exact && approximate unlearning`       | `云上模型训练中植入有毒样本，持续学习场景下发送unlearning请求激活后门` | `后门攻击`                                   | `黑盒，无需辅助数据`                                         | `伪装样本加入高斯噪声`                                       | `https://github.com/momalab/ReVeil`                        |
| `UNLEARN AND BURN: ADVERSARIAL MACHINE UNLEARNING REQUESTS DESTROY MODEL ACCURACY` | `Yangsibo Huang∗ Daogao Liu∗ Lynn Chua Badih Ghazi Pritish Kamath Ravi Kumar Pasin Manurangsi Milad Nasr Amer Sinha Chiyuan Zhang` |      `ICLR 2025`       | `approximate unlearning`                | `提交恶意的遗忘申请（修改数据）诱导服务器模型的严重性能下降` | `数据中毒攻击`                               | `黑盒 && 白盒（可以访问训练数据的子集 ，还假设攻击者知道模型使用的忘却算法--不用也可以）` | `对抗样本生成`                                               | `https://github.com/daogaoliu/unlearning-under-adversary`  |
| `UNLEARNING MAPPING ATTACK: EXPOSING HIDDEN  VULNERABILITIES IN MACHINE UNLEARNING` | `Hao Xuan, Xingyu Li`                                        |                        |                                         |                                                              |                                              |                                                              |                                                              | ❌                                                          |
| `Exposing Privacy Vulnerabilities in Machine  Unlearning via Distribution-Discrepancy Attacks` |                                                              |                        |                                         |                                                              |                                              |                                                              |                                                              |                                                            |

---

存在MIA攻击，因文献较早暂时未调研

[1]M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang, “When machine unlearning jeopardizes privacy,” in Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, 2021, pp. 896–911.

[2]N. Carlini, M. Jagielski, C. Zhang, N. Papernot, A. Terzis, and F. Tramer, “The privacy onion effect: Memorization is relative,” Advances in Neural Information Processing Systems, vol. 35, pp. 13 263–13 276, 2022.
