### 📚 Unlearning Attack 论文分类总览表

| 论文标题                                                     | 作者                                                         |  会议/年份   | 遗忘类型                                | 攻击场景 (Attack Scenario)                                   | 攻击类型 (Attack Type)                       | 威胁模型 (Threat Model)                                      | 关键方法/贡献简述                                            | 是否开源                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ | :----------: | --------------------------------------- | ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------------------------- |
| `Paper Title`                                                | `Author et al.`                                              | `Conf. Year` | `exact/approximate unlearning`          | `e.g., 单样本遗忘 / 联邦遗忘 / 在线学习`                     | `e.g., MIA / 模型提取 / 遗忘绕过 / 差分攻击` | `e.g., 黑盒 / 白盒 / 灰盒`                                   | 简要描述其攻击机制或创新点（如：“提出针对SISA框架的MIA变体”） | ✅ / ❌                                           |
| `A Duty to Forget, a Right to be Assured? Exposing  Vulnerabilities in Machine Unlearning Services` | `Hongsheng Hu∗, Shuo Wang§∗, Jiamin Chang†∗, Haonan Zhong†∗, Ruoxi Sun∗, Shuang Hao‡, Haojin Zhu§, and Minhui Xue∗` | `NDSS 2024`  | `gradient-based approximate unlearning` | `提交恶意的遗忘申请（修改数据）诱导服务器模型的严重性能下降` | `over-unlearning（对抗样本生成）`            | `黑盒(提交数据样本x来查询模型，并获得概率向量Y，但无法知道模型的参数和结构)不知道unlearnng算法` | `利用CW-attack生产对抗样本`                                  | ` https://github.com/ TASI-LAB/Over-unlearning` |
| `Backdoor Attacks via Machine Unlearning`                    | `Zihao Liu1, Tianhao Wang2, Mengdi Huai1, Chenglin Miao1`    | `AAAI 2024`  | `exact && approximate unlearning`       | `通过使用unlearning擦除其部分训练数据来使目标模型表现出后门行为` | `后门攻击`                                   | `黑盒（）&&白盒（毒害训练集）都知道unlearning算法`           | `数学公式优化推导触发器及数据集，离散的样本选择转换为连续的概率` | ❌                                               |
| `FedMUA: Exploring the Vulnerabilities of  Federated Learning to Malicious  Unlearning Attacks` | `Jian Chen , Member, IEEE, Zehui Lin, Wanyu Lin ,  Wenlong Shi, Xiaoyan Yin , Member, IEEE, and Di Wang , ` | `TIFS 2025`  | `gradient-based approximate unlearning` | `发起恶意的特征遗忘请求来显著地改变与目标样本相关的预测`     | `over-unlearning（对抗样本生成）`            | `黑盒(提交数据样本x来查询模型，并获得概率向量Y，但无法知道模型的参数和结构)不知道unlearnng算法` | `利用CW-attack生产对抗样本`                                  | ` https://github.com/ity207/FedMUA`             |
|                                                              |                                                              |              |                                         |                                                              |                                              |                                                              |                                                              |                                                 |
| `Exposing Privacy Vulnerabilities in Machine  Unlearning via Distribution-Discrepancy Attacks` |                                                              |              |                                         |                                                              |                                              |                                                              |                                                              |                                                 |

---

